---
---
@article{StochasticBandits,
    title={Stochastic Bandits with Strongly Convex Model Families},
    author={E. Shen and K. Xu and O. Bastani},
    abstract={We study the nonlinear bandit problem in the setting where the rewards are given by a parametric model family, and the loss for that model family is strongly convex. For this setting, we provide an algorithm and prove that it achieves $O(\sqrt{T})$ regret. Our bounds generalize linear bandits into the nonlinear setting.},
    year={2023},
    selected={true}
}

@misc{bastani2022regret,
    title={Regret Bounds for Risk-Sensitive Reinforcement Learning}, 
    author={O. Bastani and Y. J. Ma and E. Shen and W. Xu},
    year={2022},
    eprint={2210.05650},
    archivePrefix={arXiv},
    pdf={Risk-Sensitive.pdf},
    selected={true},
    primaryClass={cs.LG}
}

@article{BIG,
    title={Balanced excitation and inhibition could help estimate gradients},
    author={Estelle Shen, Konrad Kording, Richard Lange, Ari Benjamin.},
    abstract={In order for synaptic plasticity to improve behavior in biological systems, the direction of plasticity must depend upon how synapses affect outcomes. In theoretical neuroscience, a wide variety of algorithms have been put forward as hypotheses as to how neurons might align plasticity with the gradient of downstream outcomes, but none is unambiguously observed in cortical physiology. Here, we put forward a new hypothesis linking biologically-plausible credit assignment to gradient estimation which leverages the fact that, for many common problems, gradient estimation can be recast as score estimation, i.e. estimating the gradient of the log probability density of neuronal activity. Using principles of score estimation recently developed in the context of denoising diffusion models, we then derive a new biologically-plausible credit-assignment algorithm we call the Balanced Inhibition Gradient (BIG). In BIG, the gradient of a downstream objective is represented directly in the voltage representing the difference between feedback excitation and local inhibition. Synaptic plasticity is then the Hebbian product between presynaptic activity and this postsynaptic potential. Both excitatory feedback and local inhibition are trained to locally denoise postsynaptic activity. This algorithm matches numerous physiological observations, including the surprisingly tight balance between inhibitory and excitatary currents even on fast timescales. In sum, BIG draws on the success of gradient-based learning and score estimation in machine learning and offers a novel explanation for known features of biological neurons.},
    year={2023},
    pdf={Credit_assignment_via_EI_balance__COSYNE_.pdf},
    selected={true}
}

@poster{sesmemory,
    title={Lower Memory Ability in Lower SES Is Not Explained by Smaller Hippocampi}, 
    author={Estelle Shen, Yu Hao, Hyeokmoon Kweon, Lyle Ungar, Martha J. Farah.},
    year={2023},
    selected={true},
    primaryClass={cs.LG}
}